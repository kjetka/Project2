\subsection{The boundaries - $\rho_{max}$}

Because we used the radial Schr√∂dinger Equation our boundary conditions are given by $r\in[0,\infty)$ and since $\rho = r/\alpha$ the same boundary conditions are still relevant. That means that $\rho_{max}$ should be $\infty$ that is not possible however on a computer.

Therefore an approximation of the boundary conditions had to be made. We chose $\rho_{max} = ?? $ for all our calculations except when $\omega_r = 0.01$ then we changed it to be $\rho_{max} = 50$. The small frequency made the wave function smear out, and we needed a bigger interval to make sure we had the whole function in our calculations. 

We also noticed that the result was very dependent on $\rho_{max}$ especially if we chose a too small value. We used our knowledge of the analytical result to find a good $\rho_{max}$, but without this knowledge it would have been difficult. It is important to make sure that the boundary conditions are reflecting the reality in a good way to get the right results. 

\subsection{Efficiency of method}

As we can see in Table \ref{tab:time} the CPU time of our Jacobi's method is big compared to armadillo's eigenvalue solver function. We needed 400 mesh points to get a good accuracy for the eigenvalues and if we would have needed more it would have been a very time consuming job.

The number of similarity transformations was big and it give a picture of why the CPU time is big. The similarity transformations increase a lot with the number of mash point, so if the method converged with a lower number of mesh points, it could decrease the CPU time.  

No matter how we changed the number of mesh points and $\rho_{max}$, we could not get a better accuracy then three decimal points, with at least a reasonable CPU time. We did not try for mesh points resulting in CPU times over four minutes. In our mind, that is to long.  

This might indicate that we should have chosen another method to solve this eigenvalue problem, if it was not a project to learn about programming.

A interesting point we noticed was that our number of similarity transformations was lower then the one presented in the lectures for Jacobi's method. Here it was presented that a general, real and symmetric matrix requires $ 3n^2-5n^2 $ similarity transformations. In this specific case of a  tridiagonal matrix, figure \ref{fig:n} shows that it converges after $ 1.81n^2 $ similarity transforms. 

  This might be because we have a tridiagonal matrix, our matrix is full of zeros already, and because we exploit the fact that our matrix is symmetric when searching for the largest element, like mentioned in method. The number of similarity transformations are also dependent on the tolerance of when a number is so small that we reckon it to be zero. Increasing the tolerance would give fewer similarity transformations. 
\subsection{Unit test. Kjetil}

Good unit test? Better/other unit tests?

