\subsection{Jacobi's method}

\begin{align*}
t &= \tau \pm \sqrt{1+\tau^2}\\ 
c &= \cos \theta = \frac{1}{\sqrt{1+\tan^2\theta}} = \frac{1}{\sqrt{1+t^2}}\\
s &= \sin \theta = \tan \theta \cos \theta = t c\\
\end{align*}

\subsection{The Jacobi algorithm}

The essence of the algorithm is to continue to do similarity transforms on the matrix A until it is more or less diagonalized. Because the off-diagonal elements will approach $ 0 $, it is sufficient to count until the largest off-diagonal element is $ < 10^{-10} $. 

Our matrix \textbf{A} is symmetric, which means that $ \textbf{A}^T = \textbf{A} $. From this it follows that the product $ \textbf{S}^TAS $ also is symmetric, as is shown under:

\begin{align}
\textbf{S}^T\textbf{AS} &= \textbf{S}^T\textbf{A}^T(\textbf{S}^T)^T\\
&= \textbf{S}^T\textbf{AS} 
\end{align}

Thus  it is sufficient to search only the elements above the diagonal for the highest valued element in the matrix, instead of the entire matrix. This greatly speeds up the searching-algorithm, as you half the number of elements to read in order to find the greatest value. 

Depending on the sign of the $ \pm $ in equation \ref{eq: t} one either gets a relatively high value for t or a small one. We chose to only use the smallest solution of that equation, see snippet below.  This means that we choose the c  which will be closest to unity  and the value of $s$ which will be the smallest, see equations \ref{eq: c} and \ref{eq: s}. This against means we each time chose the similarity transform that rotates the matrix A the most. 


\begin{lstlisting}[caption={Determining $t$ in the program},label=kode]
if(tau>= 0)
	t = 1.0/(tau + sqrt(1+tau*tau));

else
	t = -1.0/(- tau + sqrt(1+tau*tau));
\end{lstlisting}

A computer has a finite number of digits to describe a number. For high values of $ \tau $  equation \ref{eq: t} will turn to $ t = \tau \pm \tau $, as $ \sqrt{1+\tau^2} \simeq \tau $ for large $ \tau $. To minimize this effect we rewrote the equation: 

\begin{align}
t &= \frac{(-\tau \pm \sqrt{1+\tau^2})(-\tau \mp \sqrt{1+ \tau^2})}{-\tau \mp \sqrt{1+\tau^2}}\\
&= \frac{-1}{-\tau \mp \sqrt{1+\tau^2}}
\end{align}


We chose to stop the algorithm when all the off-diagonal elements in A where smaller than $ \epsilon = 10^{-10} $.  For greater accuracy this number could be smaller, but this gives a decent payoff between time and accuracy. 

The strength of the oscillator potential is reflected in $ \omega_r $. If $ \omega_r $ is small, the wave function will be less confined and will thus need a higher $ \rho_{max} $ than for strongly confined wave functions. This leads to choosing  $   \rho_{max} = 50 $ for $  \omega<0.1 $ and $ \rho_{max} = 7.9 $ for the other values of $ \omega_r $. 


\subsection{Unit tests}

In this project we used two unit tests to ensure that the program performs as expected and delivers accurate enough results. 

One way to be sure that our algorithm gives correct answers is to task it to find the eigenvalues of a matrix with known eigenvalues. These values had been pre calculated by Matlab. 

The other unit test we utilized was to check that our algorithm to find the largest off-diagonal elements actually found the largest off-diagonal elements. This was done by setting up a known matrix and tasking our algorithm to find the largest off-diagonal element and checking it against the manually found largest element. 





